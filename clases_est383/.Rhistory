library(ff)
library(ffbase)
library(bigmemory)
library(biganalytics)
mr<-matrix(rnorm(10000000*10),nrow = 10000000,ncol=10)
mr<-cbind(mr,binom=rbinom(10000000,5,0.7))
library(parallel)
#install.packages("snow")
library(snow)
#funciones propias
mclapply()#no funciona completamente en windows
parLapply()
library(help=parallel)
#viendo los cores disponibles
detectCores()
#definiendo un cluster de trabajo
#SOCK usa Rscript para lanzar más copias de R (en el mismo host u opcionalmente en otro lugar)(internamente)
#FORK makeForkCluster bifurca a los trabajadores en el host actual (que hereda el entorno de la sesión actual)  (NO WINDOWS)
#bifurcar: el trabajo lo reparte a los cores
cl <- makeCluster(3, type = "SOCK")
stopCluster(cl)
cl <- makeCluster(2, type = "SOCK")
teval(parApply(cl,mr,2,mean))
stopCluster(cl)
teval(
for(i in 1:10){
for(j in unique(mff[,11])){
aux[j+1,i]<-mean(mff[mff[,11]==j,i])
}
}
)
teval<-function(...){
gc()
start<-proc.time()
result<-eval(...)
finish<-proc.time()
return(list(Duration=finish-start,Result=result))
}
cl <- makeCluster(2, type = "SOCK")
teval(parApply(cl,mr,2,mean))
stopCluster(cl)
cl <- makeCluster(3, type = "SOCK")
teval(parApply(cl,mr,2,mean))
stopCluster(cl)
cl <- makeCluster(4, type = "SOCK")
teval(parApply(cl,mr,2,mean))
stopCluster(cl)
A<-matrix(10^6,10)
B<-matrix(10^6,10)
C<-matrix(10^6,10)
mclapply(X=list(A,B,C),FUN=mean,mc.preschedule = F,affinity.list = c(1,1,2))
mclapply(X=list(A,B,C),FUN=mean)
system("java -version")
system("java -version")
#Habilitando la librería
library(sparklyr)
#versiones disponibles
spark_available_versions()
#versiones instaladas
spark_installed_versions()
#inicia la sesión
sc <- spark_connect(master = "local", version = "2.4.3")
spark_disconnect(sc)
knitr::opts_chunk$set(echo = TRUE)
#Habilitando la librería
library(sparklyr)
#inicia la sesión
sc <- spark_connect(master = "local", version = "2.4.3")
spark_disconnect(sc)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
#spark_home="C:\\Users\\ALVARO\\AppData\\Local\\spark\\spark-2.4.3-bin-hadoop2.7"
#conf$sparklyr.gateway.port <- 9090
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
#apagar conexión
spark_disconnect(sc)
spark_disconnect_all()
#spark_home="C:\\Users\\ALVARO\\AppData\\Local\\spark\\spark-2.4.3-bin-hadoop2.7"
#conf$sparklyr.gateway.port <- 9090
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
spark_web(sc)
spark_disconnect_all()
